{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ExaOcnjFil"
      },
      "source": [
        "# CSE 6240/CX4803 Web Search and Text Mining\n",
        "## Homework 3: Centrality and Pagerank\n",
        "\n",
        "This homework allows you to explore with centrality and pagerank algorithms on a real world social network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np3PIM2bzDqv",
        "outputId": "ad1e3460-7ac8-4c4a-e9a7-18465e8437b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting networkx==2.6.3\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.1\n",
            "  Downloading scipy-1.7.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy==1.7.1) (1.21.6)\n",
            "Installing collected packages: scipy, networkx\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "Successfully installed networkx-2.6.3 scipy-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx==2.6.3 scipy==1.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7RbCrcMgk3J"
      },
      "source": [
        "Please run the following cell substituting your student and user names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7clA8ohorzLd"
      },
      "outputs": [],
      "source": [
        "## Do not change or add any other libraries ## \n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from scipy.stats import pearsonr\n",
        "import scipy.sparse as sp\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHA5XXogr7b7",
        "outputId": "784c00ff-b506-43ec-8363-2711911527c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I, Yeojin Chang (ychang354), state that I performed the tasks in this assignment following the Georgia Tech honor code (https://osi.gatech.edu/content/honor-code).\n"
          ]
        }
      ],
      "source": [
        "def author_honor_code (student_name, user_name):\n",
        "  print (f'I, {student_name} ({user_name}), state that I performed the tasks in this assignment following the Georgia Tech honor code (https://osi.gatech.edu/content/honor-code).')\n",
        "\n",
        "# print the honor code before submission (substitute your name and username)\n",
        "author_honor_code (student_name='Yeojin Chang', user_name='ychang354')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lsA_MN2ra8S"
      },
      "source": [
        "**Important NOTE: To remove any randomness, please make sure to pass the parameter `v0` (i.e., initial estimate for the eigenvector) to any call to scipy.linalg.eigsh as the all-ones vector of size (N,), where N is the number of nodes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzcDHVAvXirt"
      },
      "source": [
        "# Data Loading: GitHub network [0.1 points]\n",
        "\n",
        "Throughout this assignment, we will use a large social network from [GitHub](https://github.com). The following cell will download the network in your current directory. If it does not work, please download the zip file from [here](http://snap.stanford.edu/data/git_web_ml.zip) and unzip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IBD4RmXYyiJ"
      },
      "source": [
        "The GitHub network can be described as follows:\n",
        "\n",
        "- A node in the network represents a developer account that is active.\n",
        "- Two nodes are linked if they mutually follow each other.\n",
        "\n",
        "There is other information related to the nodes in the data you just downloaded, but for the purpose of this assignment we will ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nLQSNBiYYYqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e36e715-4e8e-4a1c-8091-cc0d9f743b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 16:04:54--  http://snap.stanford.edu/data/git_web_ml.zip\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2396031 (2.3M) [application/zip]\n",
            "Saving to: ‘git_web_ml.zip’\n",
            "\n",
            "git_web_ml.zip      100%[===================>]   2.28M  2.21MB/s    in 1.0s    \n",
            "\n",
            "2023-02-22 16:04:55 (2.21 MB/s) - ‘git_web_ml.zip’ saved [2396031/2396031]\n",
            "\n",
            "Archive:  git_web_ml.zip\n",
            "   creating: git_web_ml/\n",
            "  inflating: git_web_ml/musae_git_edges.csv  \n",
            "  inflating: git_web_ml/musae_git_features.json  \n",
            "  inflating: git_web_ml/musae_git_target.csv  \n",
            "  inflating: git_web_ml/citing.txt   \n",
            "  inflating: git_web_ml/README.txt   \n"
          ]
        }
      ],
      "source": [
        "![ ! -d \"git_web_ml\" ] && wget http://snap.stanford.edu/data/git_web_ml.zip && unzip git_web_ml.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jlcmU9zPJ_uR"
      },
      "outputs": [],
      "source": [
        "def load_net (filename):\n",
        "  \"\"\" Load the network from the file.\n",
        "  Arguments\n",
        "  ---------\n",
        "  filename (str): The name of the file which \n",
        "  contains the edges.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  networkx.Graph\n",
        "\n",
        "  Steps:\n",
        "  1. Split each line by comma.\n",
        "  2. Add edges to the network.\n",
        "  \"\"\"\n",
        "  net = nx.Graph()\n",
        "  with open (filename) as fin:\n",
        "    for i, line in enumerate (fin):\n",
        "      if i > 0:\n",
        "        ## Add code below [0.1 points] ##\n",
        "        e = line.split(',')\n",
        "        net.add_edge(e[0], e[1][:-1])\n",
        "        #################################\n",
        "  return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qzV6VvvJiY7u"
      },
      "outputs": [],
      "source": [
        "## Do not change ## \n",
        "net = load_net ('git_web_ml/musae_git_edges.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABzvkHsPzDq2",
        "outputId": "0a53aa9a-a6a2-4a29-99be-a215c608f5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "## Do not change. This should print True if your code is correct. ## \n",
        "print ((net.number_of_nodes() == 37700) and (net.number_of_edges() == 289003))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNfxqaiiwG-i"
      },
      "source": [
        "# Section 1: Centrality measures [0.4 points]\n",
        "\n",
        "A common question in network analysis is to find the central nodes in the network. These nodes are likely to be more important and more influential. In this section, we'll first calculate the degree, betweenness, and eigenvector centrality for the GitHub network by calling `networkx` functions. \n",
        "\n",
        "The betweenness centrality calculation can be extremely slow. To speed it up, please use $100$ node samples to approximate the value instead of all the nodes. This can be achieved by the networkx function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pndpUxIWV80H"
      },
      "outputs": [],
      "source": [
        "random.seed (42)\n",
        "np.random.seed (42)\n",
        "## Add code to calculate the different centrality measures [0.2 points] ##\n",
        "degree_centrality = nx.degree_centrality(net)\n",
        "betweenness_centrality = nx.betweenness_centrality(G = net, k = 100)\n",
        "eigenvector_centrality = nx.eigenvector_centrality(net)\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mB8OvUVGZV3t"
      },
      "outputs": [],
      "source": [
        "# Do not change\n",
        "def get_top_central_nodes (centrality_scores, top_k=10):\n",
        "  \"\"\" Given the centrality scores dict, give the top_k\n",
        "      nodes with the highest centrality\n",
        "  \"\"\"\n",
        "  return np.array(list(sorted (centrality_scores.keys(), key=lambda x:centrality_scores[x], reverse=True))[:top_k])\n",
        "\n",
        "top5_deg = get_top_central_nodes (degree_centrality, top_k=5)\n",
        "top5_bw = get_top_central_nodes (betweenness_centrality, top_k=5)\n",
        "top5_eig = get_top_central_nodes (eigenvector_centrality, top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K3ILu9CizDq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6973b840-8e2f-4c7d-bcc3-263842f3abfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code is correct.\n",
        "print (np.all(top5_deg == top5_eig) and (np.all(top5_bw[[1,0,3,2,4]] == top5_deg)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ELnoi2ZZrO"
      },
      "source": [
        "Now calculate the Pearson's correlation between these centrality metrics using `pearsonr` function from `scipy.stats`\n",
        "\n",
        "Note: Make sure that the input to the functions are the centrality values for the same sequence of nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1TvP0ZKzZZ4Y"
      },
      "outputs": [],
      "source": [
        "## Add code to calculate the pearson correlation between the different centrality measures [0.1 points] ##\n",
        "corr_deg_bw = pearsonr(list(degree_centrality.values()), list(betweenness_centrality.values()))[0]\n",
        "corr_deg_eig = pearsonr(list(degree_centrality.values()), list(eigenvector_centrality.values()))[0]\n",
        "corr_bw_eig = pearsonr(list(betweenness_centrality.values()), list(eigenvector_centrality.values()))[0]\n",
        "##########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0hdacXrozDq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877b9e15-961e-4804-f988-4ed9106c6b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code above is correct.\n",
        "print (np.all (np.abs([corr_deg_bw - 0.873, corr_bw_eig - 0.643, corr_deg_eig - 0.858]) <= 5e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpK7UP4hZdH9"
      },
      "source": [
        "Explain why certain centrality measures are more correlated than others ? [0.1 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCThhiRfZjJI"
      },
      "source": [
        "> In this graph, we can observe that the correlation between degree centrality and betweennesss centrality and the correlation between degree centrality and eignvector centrality is significantly higher than that of betweenness centrality and eignvector centrality. This is because the calculations for betweenness centrality and eigenvector centrality somewhat depend on the degrees of a node. For instance, if a node has a lot of edges leading in/out of the node (degree centrality), it is more likely to see more information when the shortest path is taken (betweenness centrality). However, betweenness and eigenvector centrality don't necessarily have this correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_F77afKPiJq"
      },
      "source": [
        "# Section 2: HITS and PageRank  [6.5 points]\n",
        "\n",
        "In this section, we'll then move to link analysis and implement vectorized (hence, scalable) versions of the HITS and PageRank algorithm to find the more important nodes in the network. Finally, we'll compare our PageRank scores with `networkx` Pagerank scores and scores from the HITS algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzXiUrh5zDq5"
      },
      "source": [
        "\n",
        "To get started, we will first convert the network into a directed network\n",
        "(This is not strictly necessary, but HITS and PageRank work well on a directed network). We will simply convert the edges to directed edges by making them point to nodes that have a lower id.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_O7opO1lW9ot"
      },
      "outputs": [],
      "source": [
        "# Do not change. Function to convert to directed graph\n",
        "def convert_to_directed (net):\n",
        "  G = nx.DiGraph ()\n",
        "  for source, target in net.edges ():\n",
        "    if int (source) > int (target):\n",
        "      G.add_edge (source, target)\n",
        "    else:\n",
        "      G.add_edge (target, source)\n",
        "  \n",
        "  return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1UHg7DXeb_52"
      },
      "outputs": [],
      "source": [
        "# Get directed graph. Do not change.\n",
        "di_net = convert_to_directed (net)\n",
        "A = nx.linalg.graphmatrix.adjacency_matrix (di_net)\n",
        "nodeid2rownum = {node: i for i, node in enumerate (di_net.nodes())}\n",
        "rownum2nodeid = {i: node for i, node in enumerate (di_net.nodes())}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8cTgpbpzDq5"
      },
      "source": [
        "## HITS algorithm [3.0 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMP5GThgEkUT"
      },
      "source": [
        "We will implement HITS in 3 different ways: (i) Matrix Multiplication Iterative method, (ii) NetworkX functions, and (iii) Eigenvector method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq8MEyn7gb7r"
      },
      "source": [
        "### Matrix Multiplication Iterative HITS algorithm [2.5 points]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhWPYZHBzDq6"
      },
      "source": [
        "\n",
        "You will implement the matrix formulation of HITS algorithm as given by the following iterative algorithm.\n",
        "\n",
        "$$\n",
        "\\vec{h}^{(0)} = 1, \\\\\n",
        "\\vec{a}^{(0)} = 1, \\\\ \n",
        "\\\\\n",
        "\\vec{h}^{(t+1)} = c_1 A \\vec{a}^{(t)}, \\\\\n",
        "\\vec{a}^{(t+1)} = c_2 A^T \\vec{h}^{(t)},\n",
        "$$\n",
        "\n",
        "where $\\vec{a}^{(t)}, \\vec{h}^{(t)}$ are the authority and hub scores of the nodes at time $t$; $A$ is the adjacency matrix of the graph. And $c_1, c_2$ are some constants. \n",
        "\n",
        "\n",
        "After each iteration, it is important to normalize the authority and hub scores over all the nodes to make the algorithm converge by keeping the scores bounded within $1$. In particular, we do the following:\n",
        "\n",
        "$$\n",
        "\\vec{a}^{(t)} = \\vec{a}^{(t)}/\\lVert \\vec{a}^{(t)} \\rVert \\\\\n",
        "\\vec{h}^{(t)} = \\vec{h}^{(t)}/\\lVert \\vec{h}^{(t)} \\rVert\n",
        "$$\n",
        "\n",
        "Note that you don't need to explicitly set $c_1, c_2$ as the normalization will take care of it.\n",
        "\n",
        "Note that we are asking you to use matrix multiplication methods, instead of summing the neighbors' hub or authority values. The latter is slow. Matrix multiplication methods are much faster. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Eo757fbHzDq6"
      },
      "outputs": [],
      "source": [
        "def my_hits (G, A, num_iters=50, tol=1e-3):\n",
        "  \"\"\" Implement the matrix multiplication-based iterative HITS algorithm\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  G (networkx.DiGraph): The directed network for which pagerank is to be calculated.\n",
        "  A (scipy.sparse.matrix): The adjacency matrix.\n",
        "  num_iters (int): The max number of iterations.\n",
        "  tol (float): The tolerance limit. Exit the algorithm if num_iters is reached or scores from \n",
        "               current iteration differ from the previous iteration only upto tolerance specified.\n",
        "  Returns\n",
        "  -------\n",
        "  hub_scores, auth_scores: Two dictionaries storing hub and authority scores keyed by the nodes in the graph.\n",
        "\n",
        "  ###########################################\n",
        "  ## Steps:\n",
        "  # 1. Initialize the hub and auth scores. Each vector should be initialized for each node with a score of 1. \n",
        "  # 2. For num_iters iterations, iteratively update the auth and hub scores according to the above equation. \n",
        "  # 3. In each iteration, calculate new hub and authority score vectors from current authority and hub score vectors, respectively. \n",
        "  #    Use the appropriate matrix multiplication operations.\n",
        "  # 4. In each iteration, normalize the new vectors. Use the np.linalg.norm function appropriately. \n",
        "  # 5. In each iteration, check for exit condition. Note that the exit condition should ensure tolerance limit is satisfied for both hub and authority scores.\n",
        "  #    -> Calculation of tolerance should be done by computing the mean sqaured error (MSE) between the previous and current value.\n",
        "  # 6. After all the iterations are completed, create the hub_scores and auth_scores dictionaries. Each dictionaries has node id as key and hub or authority score as value. \n",
        "  \n",
        "  ## Notes for faster implementation:\n",
        "  # - Implement the vectorized versions of the above equations so that the code can efficiently run for 100 iterations. \n",
        "  #   Summing up neighbors' scores is slow and should not be used. \n",
        "  # - For faster implementation, you can compute the new and old authority and hub scores using matrix multiplications. Note that this will be a multiplication between a \n",
        "  #   scipy sparse matrix and a numpy array vector. Refer https://docs.scipy.org/doc/scipy/reference/sparse.html#matrix-vector-product or \n",
        "  #   you may directly use the @ (or matrix multiplication) operation. \n",
        "  # - Make sure that normalization is also done in a vectorized manner. \n",
        "  \"\"\"\n",
        "  hub_scores, auth_scores = None, None\n",
        "  # Add code below\n",
        "  \n",
        "  # step 1: initialize\n",
        "  num_nodes = di_net.number_of_nodes()\n",
        "  hub_scores = np.full(num_nodes, 1)\n",
        "  auth_scores = np.full(num_nodes, 1)\n",
        "\n",
        "  # step 2: iterate\n",
        "  for i in range(num_iters):\n",
        "    # step 3: update\n",
        "    hub_t = hub_scores\n",
        "    auth_t = auth_scores\n",
        "    hub_scores = A@auth_t\n",
        "    auth_scores = A@hub_t\n",
        "\n",
        "    # step 4: normalize\n",
        "    hub_scores = hub_scores/np.linalg.norm(hub_scores)\n",
        "    auth_scores = auth_scores/np.linalg.norm(auth_scores)\n",
        "\n",
        "    # step 5: exit condition\n",
        "    if sum(np.square(hub_scores - hub_t)) < tol:\n",
        "      hub_scores = {rownum2nodeid[i]: hub_scores[i] for i, node in enumerate(G.nodes())}\n",
        "      auth_scores = {rownum2nodeid[i]: auth_scores[i] for i, node in enumerate(G.nodes())}\n",
        "      return hub_scores, auth_scores\n",
        "\n",
        "    if sum(np.square(auth_scores - auth_t)) < tol:\n",
        "      hub_scores = {rownum2nodeid[i]: hub_scores[i] for i, node in enumerate(G.nodes())}\n",
        "      auth_scores = {rownum2nodeid[i]: auth_scores[i] for i, node in enumerate(G.nodes())}\n",
        "      return hub_scores, auth_scores\n",
        "\n",
        "    \n",
        "\n",
        "  hub_scores = {rownum2nodeid[i]: hub_scores[i] for i, node in enumerate(G.nodes())}\n",
        "  auth_scores = {rownum2nodeid[i]: auth_scores[i] for i, node in enumerate(G.nodes())}\n",
        "\n",
        "  ####################################################\n",
        "\n",
        "  return hub_scores, auth_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zeeZdV4WlcFp"
      },
      "outputs": [],
      "source": [
        "# %%time \n",
        "# Do not change. An efficient solution would be able to run this within one second. \n",
        "myhub_scores, myauth_scores = my_hits(di_net, A, num_iters=100, tol=1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEm0VJZ3q6Z7"
      },
      "source": [
        "### Calculating HITS with Networkx [0.1 points]\n",
        "Now, let's check how your `my_hits` perform as compared to the networkx's implementation of the iterative algorithm. \n",
        "\n",
        "\n",
        "Calculate the hubs and authority scores using built-in `networkx` hits function. You're allowed to use `nx` functions.  Use default parameters for num_iters and tol (which are same as for the call to my_hits above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WI-QNdxuQCFj"
      },
      "outputs": [],
      "source": [
        "## Add code below to get hub and authority scores from networkx [0.1 points] ##\n",
        "# This should return hub and authority scores as key-value pairs in a dictionary where keys are nodes.\n",
        "nxhub_scores, nxauth_scores = nx.hits(G=di_net, max_iter=100, tol=1e-8)\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ozoG0I57zDq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5725102-0e06-4ab0-8110-50750995b9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code above is correct.\n",
        "hub_diff = np.array([np.abs(nxhub_scores[x] - myhub_scores[x]) for x in myhub_scores])\n",
        "auth_diff = np.array([np.abs(nxauth_scores[x] - myauth_scores[x]) for x in myauth_scores])\n",
        "\n",
        "print (auth_diff.mean() <= 5e-3 and hub_diff.mean() <= 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBalpFK3zDq7"
      },
      "source": [
        "Thus, the difference is small and implementation is good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aO5H70CzDq7"
      },
      "source": [
        "### Calculating HITS with Eigenvectors [0.4 points] \n",
        "\n",
        "We know that the iteration in my_hits converges as $t \\rightarrow \\infty$. \n",
        "\n",
        "Now, let us calculate how close the calculated iterative values are to eigenvector-based HITS values. Recall that eigenvector-based HITS values are the final values at convergence.  \n",
        "\n",
        "You will use scipy.linalg libraries to calculate eigenvectors of matrices. Then, you will measure how close the values obtained by `my_hits` function are to the eigenvector methods. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZY_Jv3BDzDq7"
      },
      "outputs": [],
      "source": [
        "## Write code to find the authority and hub scores by finding the appropriate eigenvectors ##\n",
        "# Notes:\n",
        "#  - Calculate the principal eigenvector of the appropriate matrices to calculate the authority and hub scores respectively. \n",
        "#    Use sp.linalg.eigsh function to calculate eigenvectors. Please make sure to pass the parameter `v0` \n",
        "#    (initial estimate for the eigenvector) as the all-ones vector of size (N,), where N is the number of nodes. \n",
        "#  - Again, use the @ (or matrix multiplication) operation for faster computations.\n",
        "#  - Note that true_auth_scores and true_hub_scores should be an array of dimension (N,) and type float (where N is the number of nodes)\n",
        "true_auth_scores, true_hub_scores = None, None\n",
        "# Add code below:\n",
        "# \n",
        "n = A.shape[0]\n",
        "ata = sp.linalg.eigsh(A = (A.T@A).astype(float), v0=np.full(n, float(1.0)))\n",
        "true_auth_scores = ata[1][:, 0]\n",
        "\n",
        "aat = sp.linalg.eigsh(A = (A@A.T).astype(float), v0=np.full(n, float(1.0)))\n",
        "true_hub_scores = aat[1][:, 0]\n",
        "\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Nt3951VSzDq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc948c26-498b-4cbf-be3f-3444f5a1e10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code above is correct.\n",
        "hvec = np.array([myhub_scores[rownum2nodeid[i]] for i in range(A.shape[0])])\n",
        "avec = np.array([myauth_scores[rownum2nodeid[i]] for i in range(A.shape[0])])\n",
        "\n",
        "print (np.abs(true_auth_scores - avec).mean() <= 6e-3) and (np.abs(true_hub_scores - hvec).mean() <= 2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls95pjy0zDq7"
      },
      "source": [
        "## PageRank algorithm [3.2 points]\n",
        "\n",
        "We will implement PageRank in 3 different ways: (i) Matrix Formulation of Iterative method, (ii) NetworkX functions, and (iii) Eigenvector method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJZXUAPGIqR"
      },
      "source": [
        "### Matrix Formulation of Iterative PageRank algorithm [2.5 points]\n",
        "\n",
        "Now, we will calculate Pagerank. The idea behind Pagerank is that a node is more important if other important nodes link to it. It's also the basic algorithm upon which Google ranked webpages for their search results. We'll implement a simplified iterative version of Pagerank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acgxzWq7m8g6"
      },
      "source": [
        "You'll implement the matrix formulation of PageRank as the following iterative algorithm. \n",
        "\n",
        "$$\n",
        "\\vec{r}^{(0)} = (1/N, 1/N, \\cdots, 1/N)^T \\\\\n",
        "\\vec{r}^{(t+1)} = \\frac{1-p}{N} + p M \\vec{r}^{(t)}, \n",
        "$$\n",
        "\n",
        "where $\\vec{r}^{(t)}$ is the PageRank scores of all the nodes at iteration $t$; $N$ is the total number of nodes in the network; $p$ is the transportation probability; $M$ is the stochastic random matrix defined as \n",
        "\n",
        "$$ M_{ij} = \\begin{cases} 1/d^{out}_j &; (j, i) \\text{ is an edge} \\\\ 0 &; \\text{otherwise} \\end{cases},$$\n",
        "\n",
        "where $d^{out}_j$ is the out-degree of node $j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5-tFBxR_2-m0"
      },
      "outputs": [],
      "source": [
        "def get_stochastic_matrix (A):\n",
        "  \"\"\"\n",
        "  Find the stochastic matrix M from the adjacency matrix A, for the pagerank score calculation. Use the above definition.\n",
        "  Here, we list two possible ways of implementation and you are welcome to adopt any. \n",
        "  STEPS:\n",
        "  # 1. Use A.nonzero() to obtain nonzero row and column indices of the adjacency matrix A. \n",
        "  # 2. Then use row, col to obtain the non-zero indices of M.\n",
        "  # 3. Create a data array that stores the corresponding out-degrees of nodes with the same length as row and col arrays. \n",
        "  #    Make sure that the indices for row, col match with the d^{out}_j as the definition above. \n",
        "  # 4. Use sp.csr_matrix ((data, (row, column))) [https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html] \n",
        "  #    to find M. Note that M should be of the same size as A (hint: use the shape parameter of the `csr_matrix` function). \n",
        "  This is worth 1.25 points out of 2.5 points.\n",
        "  \"\"\"\n",
        "  M = None\n",
        "  # Add code below\n",
        "  #\n",
        "  n = A.shape[0]\n",
        "  row_nonzeros, col_nonzeros = A.nonzero()\n",
        "  dat = [1/np.count_nonzero(col_nonzeros==i) for i in col_nonzeros]\n",
        "\n",
        "  M = sp.csr_matrix((dat, (row_nonzeros, col_nonzeros)), shape=(n, n))\n",
        "  ####################################################\n",
        "  return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "64fNuJKQzDq7"
      },
      "outputs": [],
      "source": [
        "def my_pagerank (G, A, p=0.85, num_iters=50, tol=1e-3):\n",
        "  \"\"\"\"\n",
        "  Calculate the pagerank scores for a given graph.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  G (networkx.DiGraph): The directed network for which pagerank is to be calculated.\n",
        "  A (scipy.sparse.matrix): The adjacency matrix for the given directed network.\n",
        "  p(float): p is the probability that a link is followed in a random walk. 1-p is the restart probability.\n",
        "  num_iters (int): The number of iterations to perform\n",
        "  tol (float): The tolerance value to keep track; algorithm should exit if \n",
        "               either the number of iterations are exhausted or difference between\n",
        "               the pagerank scores across two consecutive iterations is within tolerance.\n",
        "  Returns\n",
        "  -------\n",
        "  pr_scores (dict): The pagerank scores as key-value pairs in a dictionary where keys are nodes.\n",
        "  ###########################################\n",
        "  ## Steps:\n",
        "  # 1. Initialize the pr_scores for each node as 1/N, where N is the total number of nodes.\n",
        "  # 2. For num_iters iterations, iteratively update the pr_scores according to the above equation using M.\n",
        "  # 3. In each iteration, calculate the new page rank scores from the current page rank scores and a uniform vector 1/N for teleportation.\n",
        "  #    Use the correct matrix operations to calculate the new page rank scores. \n",
        "  # 4. In each iteration, check for exit condition. Recall that the loop exists when change in pagerank scores in iterations is less than tolerance value. \n",
        "  # 5. After all the iterations are completed, create the pr_scores dictionaries. Each dictionaries has node id as key and page rank score as value. \n",
        "  \n",
        "  ## Notes:\n",
        "  # - Implement the vectorized versions of the above equations so that the code can efficiently run for \n",
        "  #   100 iterations. Summing up neighbors' scores would be slow and should not be used. \n",
        "  # - Do not forget to check the tolerance condition at every step.\n",
        "  # - Make sure that the returned scores are dictionaries storing pagerank scores keyed by the nodes. \n",
        "  \"\"\"\n",
        "\n",
        "  pr_scores = None\n",
        "  M = get_stochastic_matrix (A)\n",
        "  # Add code below\n",
        "  # \n",
        "  # step 1\n",
        "  N = G.number_of_nodes()\n",
        "  pr_scores = np.full(N, 1/N)\n",
        "\n",
        "  # step 2\n",
        "  for i in range(num_iters):\n",
        "    old_pr = pr_scores\n",
        "    mcalc = M@old_pr\n",
        "    pr_scores = (1 - p)/N + p*mcalc\n",
        "\n",
        "    if sum(abs(pr_scores - old_pr)) < tol:\n",
        "      pr_scores = {rownum2nodeid[i]: pr_scores[i] for i, node in enumerate(G.nodes())}\n",
        "      return pr_scores\n",
        "\n",
        "  pr_scores = {rownum2nodeid[i]: pr_scores[i] for i, node in enumerate(G.nodes())}\n",
        "\n",
        "  ####################################################\n",
        "  \n",
        "  return pr_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9vpG_RcVzDq8"
      },
      "outputs": [],
      "source": [
        "# Do not change. An efficient solution should run within half a second.\n",
        "mypr_scores = my_pagerank (di_net, A, p=0.85, num_iters=100, tol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksuT4j3PzDq8"
      },
      "source": [
        "### Calculating PageRank with Networkx [0.1 points]\n",
        "Now, let's check how your `my_pagerank` perform as compared to the networkx's implementation of the iterative algorithm. \n",
        "\n",
        "Calculate the pagerank scores using the built-in `networkx` pagerank function. You're allowed to use `networkx` functions. Use default parameters for p, num_iters and tol (which are same as for the call to my_pagerank above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hk1st7XHG40r"
      },
      "outputs": [],
      "source": [
        "## Add code below to get pageranks scores from networkx ##\n",
        "# This should return pagerank scores as key-value pairs in a dictionary where keys are nodes. Use the correct networkx function. \n",
        "nxpr_scores = nx.pagerank(di_net, alpha=0.85)\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cxS4Cf4VzDq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963e760e-d88d-46d6-9316-62fa2c59b33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code above is correct.\n",
        "pr_diff = np.array([np.abs(nxpr_scores[x] - mypr_scores[x]) for x in mypr_scores])\n",
        "\n",
        "print (pr_diff.mean() <= 5e-5) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IlYUOUxzDq8"
      },
      "source": [
        "Thus, the difference is very small and your implementation is good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JDRVHnazDq8"
      },
      "source": [
        "### Calculating PageRank with Eigenvectors [0.6 points] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mhY396gzDq8"
      },
      "source": [
        "We know that the above iteration converges as $t \\rightarrow \\infty$. Now, we will compare how close the scores are to the eigenvector-based PageRank values. Refer to the slides to calculate PageRank using the Eigenvector computation. Use linalg libraries of scipy/numpy. Then, compare values obtained by `my_pagerank` function to these eigenvector values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5GoAd8zDq8"
      },
      "source": [
        "**Pagerank convergence without random teleports**: First, let us assume p = 1 and find the pagerank. This simplifies the matrix you need to use for computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "etid9HfGzDq9"
      },
      "outputs": [],
      "source": [
        "## Write code to find pagerank scores for p=1 that will be reached as number of iterations approaches infinity ##\n",
        "# STEPS:\n",
        "#  - Create the sparse stochastic matrix M. \n",
        "#  - Find its principal eigenvector (use sp.linalg.eigsh). Please make sure to pass the parameter `v0` \n",
        "#    (initial estimate for the eigenvector) as the all-ones vector of size (N,), where N is the number of nodes. \n",
        "# NOTES:\n",
        "#  - Note that since p = 1, there is no teleportation. So, use the approriate matrix for the eigenvector computation.  \n",
        "#  - You should use get_stochastic_matrix. \n",
        "#  - Note that true_pr_scores1 should be an array of dimension (N,) and type float (where N is the number of nodes)\n",
        "true_pr_scores1 = None\n",
        "# Add code below\n",
        "# \n",
        "M = get_stochastic_matrix(A)\n",
        "eig = sp.linalg.eigsh(A = M.astype(float), v0=np.full(n, float(1.0)))\n",
        "true_pr_scores1 = eig[1][:, 0]\n",
        "\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "faYOmsBazDq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed5ebc9-cc26-411b-9aa4-d050c65a7654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Compare the pagerank at p=1 with corresponding fixed point.\n",
        "# Do not change. This should print True if your code above is correct.\n",
        "mypr_scores1 = my_pagerank (di_net, A, p=1.0, num_iters=100, tol=1e-6) # Note that p is set to 1.0 here\n",
        "prvec1 = np.array([mypr_scores1[rownum2nodeid[i]] for i in range(A.shape[0])])\n",
        "print (np.abs(true_pr_scores1 - prvec1).mean() <= 5e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUuuyfLAzDq9"
      },
      "source": [
        "**Pagerank convergence with random teleports**: Suppose you have a machine with RAM of 4 GB. What issues will you face when calculating pagerank scores (for an arbitrary $p < 1$) using the eigenvector method and why? [0.2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y666KoGa3L4R"
      },
      "source": [
        "The random transportations in PageRank will cause the stochastic matrix to be not sparse, as opposed to when p=1 and there is no random transportation. Since the eigenvalue decomposition of the stochastic matrix of dimensions $n \\times n$ has a complexity of $O(n^3)$ where n is the number of nodes, it would be challenging for 4 GB of RAM to perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJSssrmYwi6_"
      },
      "source": [
        "## PageRank vs HITS [0.3 points]\n",
        "Now, let's compare the obtained pagerank and HITS scores. You only need to answer questions based on the following empirical observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "odmP7YP3zDq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2b9c41-3e3e-4cac-deb6-af55e9874347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Do not change. This should print True if your code above is correct.\n",
        "histdiff = np.histogram(np.abs([myhub_scores[x] - mypr_scores[x] for x in myhub_scores]))\n",
        "print (histdiff[0][0]/len(myhub_scores) > 0.99 and histdiff[1][0] <= 1e-6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FvCGFFpzDq9"
      },
      "source": [
        "Why are 99% of the pagerank scores similar to HITS scores?  [0.1 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHt6IJA6zDq9"
      },
      "source": [
        "> 99% of the pagerank score are similar to the HITS scores because most of the nodes have a very small score that is close to 0. This is because out of over 37K nodes, the chances of a random walker visiting one particular node is very small, and the hub/authority score calculations yield a very small score as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls_M93mMzDq9"
      },
      "source": [
        "Why then may we prefer one over the other? Give two cases each when we may prefer HITS over Pagerank and vice-versa. [0.2 points]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UeDX57bzDq9"
      },
      "source": [
        "> We would prefer HITS over Pagerank if we wanted users to be able to inflate the hub and authority scores easily, since in HITS if a page has a link to all other pages, it has a very high hub score, and any page linked to this page would have a high authority score. We would also prefer HITS if we are focusing purely on the structure of the web graph, and not if their textual contents are necessarily relevant, since it's a link-based algorithm.\n",
        "On the other hand, we would prefer Pagerank over HITS if we wanted to add a level of personalization. By restricting the teleport set of nodes, Personalized Pagerank allows for the generation of recommendations in real time. We would also prefer Pagerank if we wanted a fairer system where a lay user can't just game the system, like HITS. This is because Pagerank calculates the likelihood of a user landing on a page through random walks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFWB_PmScmUY"
      },
      "source": [
        "# Section 3: Personalized PageRank [1.0 point]\n",
        "\n",
        "Personalized Pagerank is calculated by changing the teleportation set to include only a set of nodes instead of all the nodes. Let us assume a set $S$ of teleporting nodes. Then, with a probability of (1-p), the nodes may teleport to any node in $S$ with probability $1/|S|$. In other words, we get the updated stochastic matrix to find PPR as\n",
        "\n",
        "$$ M'_{ij} = \\begin{cases} p M_{ij} + (1-p)/|S| & ;i \\in S \\\\ p M_{ij} & ;\\text{otherwise}  \\end{cases}, $$\n",
        "\n",
        "where $M$ is the stochastic random matrix of pagerank. Then, the iteration for PPR scores $\\vec{r'}$ is given by $\\vec{r'}^{(i+1)} \\gets M' \\vec{r'}^{(i)}$ in the $i$ th iteration. \n",
        "\n",
        "In this section, we will study how this \"personalization\" allows us to find the stationary distribution of the page rank scores for any p efficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAesLXFezDq9"
      },
      "source": [
        "We will find the fixed point of pagerank for $p=0.85$ when $S$ is nodes with top 10 degree centrality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2G87IdQxzDq-"
      },
      "outputs": [],
      "source": [
        "from networkx.readwrite.sparse6 import n_to_data\n",
        "# Add code to find personalized pagerank scores with the teleport node set (i.e., S) as top 10 degree central nodes\n",
        "# You are supposed to use the above code to obtain the 10 top-degree nodes and store it in the variable top10_deg.\n",
        "# Assume probability of teleportation (i.e., 1-p) to be 0.15, which means p = 0.85\n",
        "# This question is worth 0.8 points.\n",
        "## STEPS:\n",
        "#     1. Find the top 10 degree central nodes using `get_top_central_nodes` and store it in top10_deg.\n",
        "#     2. Obtain the stochastic matrix M from adjacency matrix A using your previously-defined `get_stochastic_matrix`. \n",
        "#     3. Create a \"teleporting\" sparse matrix T with its (ij)th entry as 1/10 for all i in top10_deg and 0 otherwise. You may use sp.csr_matrix. \n",
        "#     4. Calculate M' using M and T. \n",
        "#     5. Return PPR scores for top10_deg, which would be the principal eigenvector of M' (use sp.linalg.eigsh). \n",
        "#        Please make sure to pass the parameter `v0` (initial estimate for the eigenvector) as the all-ones vector of \n",
        "#        size (N,), where N is the number of nodes. \n",
        "## NOTES: \n",
        "#  - Note that M is a sparse matrix as defined above.\n",
        "#  - M' is also sparse as it only adds |S|*|V| more entries in the matrix\n",
        "#  - Note that true_pprDeg_scores should be an array of dimension (N,) and type float (where N is the number of nodes)\n",
        "\n",
        "p = 0.85\n",
        "top10_deg = get_top_central_nodes(centrality_scores=degree_centrality, top_k=10)\n",
        "true_pprDeg_scores = None\n",
        "# Add code below:\n",
        "M = get_stochastic_matrix(A)\n",
        "n = A.shape[0]\n",
        "dat = np.full(10 * n, 1/10)\n",
        "\n",
        "rows_ = [[i]*n for i in top10_deg]\n",
        "flat_row = sum(rows_, [])\n",
        "cols = list(top10_deg) * n\n",
        "\n",
        "T = sp.csr_matrix((dat, (flat_row, cols)), shape=(n,n))\n",
        "M_prime = p * M + (1-p)*T\n",
        "\n",
        "true_pprDeg_scores = sp.linalg.eigsh(A = M_prime.astype(float), v0=np.full(n, float(1.0)))[1][:, 0]\n",
        "\n",
        "# \n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr6r-W4_zDq-"
      },
      "source": [
        "Let's compare these scores with the fixed point page rank scores with no random teleportation. Thus, the only difference between `true_pprDeg_scores` and `true_pr_scores1` is that the former randomly teleports to top-degree nodes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3NXNvtx3zDq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d21707-c0e7-4ff8-de6e-560bb299883d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Let's compare the pagerank at p=1 with PPR for top-10 degree teleportation nodes.\n",
        "# Do not change. This should print True if your code above is correct.\n",
        "ppr_diff = np.abs(true_pprDeg_scores - true_pr_scores1)\n",
        "maxdiff10 = [rownum2nodeid[x] for x in sorted(range(len(true_pprDeg_scores)), key=lambda x: ppr_diff[x], reverse=True)[:10]]\n",
        "print (len(set(top10_deg).difference(maxdiff10)) <= 3) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_QYAmofzDq-"
      },
      "source": [
        "Why is it expected that page rank scores are likely to differ the most for the nodes in set $S$ only ? For which other nodes do the page rank scores change by adding teleportation to top degree nodes? [0.2 points]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQKLhA7fzDq-"
      },
      "source": [
        "> The page rank scores are likely to differ the most for the nodes in Set S only because the nodes in S are given higher \"priority\" in personalized page rank, since 1-p proportion of the time, the random walker with teleport to one of these 10 nodes. The page rank scores would also change for nodes that are \"close to\" these top 10 nodes, since the likelihood that the random walker will continue its journey after teleportation onto one of these neighboring nodes will increase."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "e75189700533a6f9c77aab1809c5976db3a8fb6574f86c0d637a13ec3af6e296"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}